{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04386b3b-7f0c-4713-9dc5-314fb205e026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingestion del archivo \"person.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d7d9371-f89d-426f-b2ac-a752a34b62cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### `Paso 0 - Actualización de variables y funciones Y validaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "964333f4-29c9-4471-ab5c-0fa50d217deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.widgets.text(\"p_environment\", \"\")\n",
    "v_environment = dbutils.widgets.get(\"p_environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea6c0e13-eb83-4425-9518-5deda8970ecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"p_file_date\", \"2024-12-16\")\n",
    "v_file_date = dbutils.widgets.get(\"p_file_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbe0b806-57d6-4ce7-a292-ec1b3c9e25dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../includes/configuration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29e0f6de-1ed7-4b61-82ff-eae50a03d8e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../includes/common_functions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3d1fe89-efc3-4240-8752-08eb5d83ccfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Chama a função para validar se o arquivo a ser processado existe\n",
    "valida_arquivo(bronze_folder_path, v_file_date, \"person.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e066b43f-f739-4785-a313-dd890d3abdb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Paso 1 - Leer el archivo JSON usando \"DataFrameReader\" de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9911db2-fa4f-4953-9bb6-379d4ea43339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%fs\n",
    "#ls /mnt/moviehistoryadilmor/bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daaf2c7f-b4d8-4ec6-bbd6-f13c2c5f00d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(spark.read.option(\"header\", True).json(\"/mnt/moviehistoryadilmor/bronze/person.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e2499f-e2f9-44c3-b48f-d5bf6256b21a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "257140e8-85da-43c8-b4a6-5a5bc386c0c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Definindo o schema na variavel name_schema\n",
    "name_schema = StructType(fields=[\n",
    "    StructField(\"forename\", StringType(), True),\n",
    "    StructField(\"surname\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6e9a4b6-ecc8-438c-982d-fe0d283e6e86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Definindo o schema na variavel persons_schema\n",
    "persons_schema = StructType(fields=[\n",
    "    StructField(\"personId\", IntegerType(), False),\n",
    "    StructField(\"personName\", name_schema)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ab85ce-609c-41b0-9ecd-2f8b341750a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "persons_df = spark.read \\\n",
    "               .schema(persons_schema) \\\n",
    "               .json(f\"{bronze_folder_path}/{v_file_date}/person.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3535668c-89dc-44ba-b824-36a06dc0cbd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(persons_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff02533a-58d7-44e7-be7e-5965b786bf90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Paso 2 - Renombrar el nombre de las columnas y añadir \"ingestion_date\" y \"enviroment\"\n",
    "1. \"personid\" renombrar a \"person_id\"\n",
    "2. Agregar las columnas \"ingestion_date\" y \"Enviroment\"\n",
    "3. Agragar la columna \"name\" a partir de la concatenación de \"forename\" y \"surname\"\n",
    "4. Eliminar columnas no requeridas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb1ab0d-c4fe-47ac-8192-ddf28e08e44e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit, col, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb8fa14-2be8-4956-b122-bd5481bc8f0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "persons_with_columns_df = add_ingestion_date(persons_df) \\\n",
    "                    .withColumnRenamed(\"personId\", \"person_id\") \\\n",
    "                    .withColumn(\"enviroment\", lit(v_environment)) \\\n",
    "                    .withColumn(\"file_date\", lit(v_file_date)) \\\n",
    "                    .withColumn(\"name\", \n",
    "                                concat(\n",
    "                                    col(\"personName.forename\"), \n",
    "                                    lit(\" \"), \n",
    "                                    col(\"personName.surname\")\n",
    "                                      )\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8533fa4f-eabc-4ea8-8e9e-67aeab66836d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "persons_final_df = persons_with_columns_df \\\n",
    "                   .select(col(\"person_id\"),\n",
    "                           col(\"name\"),\n",
    "                           col(\"ingestion_date\"),\n",
    "                           col(\"enviroment\"),\n",
    "                           col(\"file_date\")\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51cd3d59-453f-4984-8239-efb015b68cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Paso 3 - Escribir datos en el datalake en formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c7cb999-561e-43ac-ac2f-e3ede473d954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Elimina partição se ela já existir\n",
    "#drop_partition_if_exists(persons_final_df, \"movie_silver\", \"persons\", \"file_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2bf3681-3658-43a7-b768-ce1099653736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#persons_final_df.write.mode(\"overwrite\").parquet(f\"{silver_folder_path}/persons/\")\n",
    "#persons_final_df.write.mode(\"append\").partitionBy(\"file_date\").format(\"parquet\").saveAsTable(\"movie_silver.persons\")\n",
    "\n",
    "#Caso a tabela não existir, cria a tabela com os dados, senão efetua um merge dos dados na tabela\n",
    "merge_delta_lake(persons_final_df, \"movie_silver\", \"persons\", silver_folder_path, \"tgt.person_id = src.person_id\", \"file_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0760b915-1b51-4e96-8f3e-c16995c738e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%fs\n",
    "#ls /mnt/moviehistoryadilmor/silver/persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "106b0e2b-5092-4902-9a23-d9941ec5bce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(spark.read.parquet(f\"{silver_folder_path}/persons\"))\n",
    "#spark.sql(\"select * from movie_silver.persons\").display()\n",
    "display(spark.sql(\"SELECT file_date, \\\n",
    "                          COUNT(1) \\\n",
    "                   FROM movie_silver.persons \\\n",
    "                   GROUP BY file_date\"\n",
    "                   )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d019908-fe52-4f40-af76-232d7ba9031e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Success\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "05.Ingestion_file_person_json",
   "widgets": {
    "p_environment": {
     "currentValue": "production",
     "nuid": "62cbda0c-c7dc-413c-a82c-dd0684d13dfe",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "p_environment",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "p_environment",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "p_file_date": {
     "currentValue": "2024-12-30",
     "nuid": "80ef383a-7a65-485f-9245-f09de1de3b99",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2024-12-16",
      "label": null,
      "name": "p_file_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2024-12-16",
      "label": null,
      "name": "p_file_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
